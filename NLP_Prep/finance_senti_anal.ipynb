{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finance_senti_anal.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP6GNmogJTknkWMCjG1E39d"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1WXviv243L2","executionInfo":{"status":"ok","timestamp":1621324706900,"user_tz":-480,"elapsed":674,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}},"outputId":"ebbab4bd-8c83-453a-e857-1231bac80055"},"source":["#https://www.kaggle.com/khotijahs1/nlp-financial-news-sentiment-analysis\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from keras.preprocessing.text import Tokenizer\n","tqdm.pandas(desc=\"progress-bar\")\n","from gensim.models import Doc2Vec\n","from sklearn import utils\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","import gensim\n","from sklearn.linear_model import LogisticRegression\n","from gensim.models.doc2vec import TaggedDocument\n","import re\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A_0UWJlL5zF9"},"source":["df = pd.read_csv('../input/sentiment-analysis-for-financial-news/all-data.csv',delimiter=',',encoding='latin-1')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqNqFossQJEK"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yafqUp-QKTh"},"source":["df.index = range(4845)\n","df['Message'].apply(lambda x: len(x.split(' '))).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yd_SsDW6QLtP"},"source":["cnt_pro = df['sentiment'].value_counts()\n","plt.figure(figsize=(12,4))\n","sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n","plt.ylabel('Number of Occurrences', fontsize=12)\n","plt.xlabel('sentiment', fontsize=12)\n","plt.xticks(rotation=90)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vcpzTmIdQM4g"},"source":["#Convert sting to numeric\n","sentiment  = {'positive': 0,'neutral': 1,'negative':2} \n","\n","df.sentiment = [sentiment[item] for item in df.sentiment] \n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhqXQG_IQOBl"},"source":["def print_message(index):\n","    example = df[df.index == index][['Message', 'sentiment']].values[0]\n","    if len(example) > 0:\n","        print(example[0])\n","        print('Message:', example[1])\n","print_message(12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mf5xxzmjQQGi"},"source":["print_message(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTAB9jHmQRT1"},"source":["from bs4 import BeautifulSoup\n","def cleanText(text):\n","    text = BeautifulSoup(text, \"lxml\").text\n","    text = re.sub(r'\\|\\|\\|', r' ', text) \n","    text = re.sub(r'http\\S+', r'<URL>', text)\n","    text = text.lower()\n","    text = text.replace('x', '')\n","    return text\n","df['Message'] = df['Message'].apply(cleanText)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIhxatnoQSiF"},"source":["df['Message'] = df['Message'].apply(cleanText)\n","train, test = train_test_split(df, test_size=0.000001 , random_state=42)\n","import nltk\n","from nltk.corpus import stopwords\n","def tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text):\n","        for word in nltk.word_tokenize(sent):\n","            #if len(word) < 0:\n","            if len(word) <= 0:\n","                continue\n","            tokens.append(word.lower())\n","    return tokens\n","train_tagged = train.apply(\n","    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.sentiment]), axis=1)\n","test_tagged = test.apply(\n","    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.sentiment]), axis=1)\n","\n","# The maximum number of words to be used. (most frequent)\n","max_fatures = 500000\n","\n","# Max number of words in each complaint.\n","MAX_SEQUENCE_LENGTH = 50\n","\n","#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n","tokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(df['Message'].values)\n","X = tokenizer.texts_to_sequences(df['Message'].values)\n","X = pad_sequences(X)\n","print('Found %s unique tokens.' % len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fxaihjk_QUil"},"source":["X = tokenizer.texts_to_sequences(df['Message'].values)\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2JG0qxwQV_t"},"source":["#train_tagged.values[2173]\n","train_tagged.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jzgSjZTQXnd"},"source":["d2v_model = Doc2Vec(dm=1, dm_mean=1, size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\n","d2v_model.build_vocab([x for x in tqdm(train_tagged.values)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGb57A4vQZuw"},"source":["%%time\n","for epoch in range(30):\n","    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n","    d2v_model.alpha -= 0.002\n","    d2v_model.min_alpha = d2v_model.alpha"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62ARHjj0QbGm"},"source":["print(d2v_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAU5g1dFQdU1"},"source":["len(d2v_model.wv.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhjCREfIQeh2"},"source":["# save the vectors in a new matrix\n","embedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n","\n","for i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n","    while i in vec <= 1000:\n","    #print(i)\n","    #print(model.docvecs)\n","          embedding_matrix[i]=vec\n","    #print(vec)\n","    #print(vec[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yv0eqDGMQgKF"},"source":["d2v_model.wv.most_similar(positive=['profit'], topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZ81i1lmQhat"},"source":["d2v_model.wv.most_similar(positive=['investment'], topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B9KAQDoQiqd"},"source":["d2v_model.wv.most_similar(positive=['broke'], topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLIDEOY3Qkc9"},"source":["from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def tsne_plot(model):\n","    \"Creates and TSNE model and plots it\"\n","    labels = []\n","    tokens = []\n","\n","    for word in d2v_model.wv.vocab:\n","        tokens.append(model[word])\n","        labels.append(word)\n","    \n","    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=250, random_state=23)\n","    new_values = tsne_model.fit_transform(tokens)\n","\n","    x = []\n","    y = []\n","    for value in new_values:\n","        x.append(value[0])\n","        y.append(value[1])\n","        \n","    plt.figure(figsize=(16, 16)) \n","    for i in range(len(x)):\n","        plt.scatter(x[i],y[i])\n","        plt.annotate(labels[i],\n","                     xy=(x[i], y[i]),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjA37i1vQn8N"},"source":["tsne_plot(d2v_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McJgLyOZQqvz"},"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Embedding\n","\n","\n","# init layer\n","model = Sequential()\n","\n","# emmbed word vectors\n","model.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n","\n","# learn the correlations\n","def split_input(sequence):\n","     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\n","model.add(LSTM(50,return_sequences=False))\n","model.add(Dense(3,activation=\"softmax\"))\n","\n","# output model skeleton\n","model.summary()\n","model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDq1YDMjQsMV"},"source":["from keras.utils import plot_model\n","plot_model(model, to_file='model.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuxOXPqNQsi9"},"source":["Y = pd.get_dummies(df['sentiment']).values\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4ilpBSwQt3f"},"source":["batch_size = 32\n","history=model.fit(X_train, Y_train, epochs =50, batch_size=batch_size, verbose = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgCQavV6QvP1"},"source":["plt.plot(history.history['acc'])\n","plt.title('model accuracy')\n","plt.ylabel('acc')\n","plt.xlabel('epochs')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","plt.savefig('model_accuracy.png')\n","\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","#plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epochs')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","plt.savefig('model_loss.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56bI-8XFQxMO"},"source":["# evaluate the model\n","_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n","_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n","print('Train: %.3f, Test: %.4f' % (train_acc, test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjAcVXkwQym2"},"source":["# predict probabilities for test set\n","yhat_probs = model.predict(X_test, verbose=0)\n","print(yhat_probs)\n","# predict crisp classes for test set\n","yhat_classes = model.predict_classes(X_test, verbose=0)\n","print(yhat_classes)\n","# reduce to 1d array\n","#yhat_probs = yhat_probs[:, 0]\n","#yhat_classes = yhat_classes[:, 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_b53PRYQ1uN"},"source":["import numpy as np\n","rounded_labels=np.argmax(Y_test, axis=1)\n","rounded_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pak-yWFDQ29N"},"source":["from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(rounded_labels, yhat_classes)\n","cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HgnonfaQ4EL"},"source":["# The confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","lstm_val = confusion_matrix(rounded_labels, yhat_classes)\n","f, ax = plt.subplots(figsize=(5,5))\n","sns.heatmap(lstm_val, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\n","plt.title('LSTM Classification Confusion Matrix')\n","plt.xlabel('Y predict')\n","plt.ylabel('Y test')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHJRtW1qQ6W-"},"source":["validation_size = 610\n","\n","X_validate = X_test[-validation_size:]\n","Y_validate = Y_test[-validation_size:]\n","X_test = X_test[:-validation_size]\n","Y_test = Y_test[:-validation_size]\n","score,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n","\n","print(\"score: %.2f\" % (score))\n","print(\"acc: %.2f\" % (acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPXNI7SlQ7xv"},"source":["model.save('Mymodel.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3Hv-Df8Q8MO"},"source":["message = ['Congratulations! you have won a $1,000 Walmart gift card']\n","seq = tokenizer.texts_to_sequences(message)\n","\n","padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n","\n","pred = model.predict(padded)\n","\n","labels = ['0','1','2']\n","print(pred, labels[np.argmax(pred)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MjVWNkEQ-Ac"},"source":["message = ['such massive asteroid hit will certainly create new business opportunities']\n","seq = tokenizer.texts_to_sequences(message)\n","\n","padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n","\n","pred = model.predict(padded)\n","\n","labels = ['0','1','2']\n","print(pred, labels[np.argmax(pred)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xt5A2zYXQ_dd"},"source":["message = ['so does anyone else not open Snapchat anymore ? or is it just me ... ugh this so sad.']\n","seq = tokenizer.texts_to_sequences(message)\n","\n","padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n","\n","pred = model.predict(padded)\n","\n","labels = ['0','1','2']\n","print(pred, labels[np.argmax(pred)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wEqYDiORBL-"},"source":["message = ['@elonmusk had a terrible experience with a very pushy sales guy from tesla Stanford shop while shopping for model x']\n","seq = tokenizer.texts_to_sequences(message)\n","\n","padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n","\n","pred = model.predict(padded)\n","\n","labels = ['0','1','2']\n","print(pred, labels[np.argmax(pred)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDNxNwEnRCaX"},"source":["message = ['The local electronics industry is expected to remain stable amid layoff concerns surrounding Japanese electronics giants operating in the country, an official says.']\n","seq = tokenizer.texts_to_sequences(message)\n","\n","padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n","\n","pred = model.predict(padded)\n","\n","labels = ['0','1','2']\n","print(pred, labels[np.argmax(pred)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppSFLSeMRDuV"},"source":["message = ['The local electronics industry is amid layoff concerns and last year has laid off tens of employees']\n","seq = tokenizer.texts_to_sequences(message)\n","\n","padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n","\n","pred = model.predict(padded)\n","\n","labels = ['0','1','2']\n","print(pred, labels[np.argmax(pred)])"],"execution_count":null,"outputs":[]}]}