{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDB_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcfH4r12ckCh","executionInfo":{"status":"ok","timestamp":1620024396972,"user_tz":-480,"elapsed":718,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}},"outputId":"7a663843-9e68-485d-891d-958160970092"},"source":["#https://peaceful0907.medium.com/%E4%BE%86%E7%8E%A9%E9%BB%9Enlp-lstm-vs-bert-on-imdb-dataset-4aa18ecd65e2\n","import os\n","import re\n","from itertools import chain\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nXA2GJud4XmD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620024630202,"user_tz":-480,"elapsed":1676,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}},"outputId":"fa0eeb8a-cea9-41ce-ecc7-befc325d95b2"},"source":["#%cd drive/MyDrive/二技資管一甲/下學期/news_dataset/\n","! ls"],"execution_count":10,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rz6jB7ap9wty","executionInfo":{"status":"ok","timestamp":1620024636469,"user_tz":-480,"elapsed":1200,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}}},"source":["#files = os.listdir(\"drive/MyDrive/二技資管一甲/下學期/news_dataset/\")\n","#print(files)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lm-tOJsjh_EV","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"error","timestamp":1620024639362,"user_tz":-480,"elapsed":908,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}},"outputId":"50d66910-ab00-44f0-8c64-2b72c0b9b99a"},"source":["#news_data = pd.read_json(\"drive/MyDrive/二技資管一甲/下學期/news_dataset/News_Category_Dataset_v2.json\", lines = True)\n","\n","TAG_RE = re.compile(r'<[^>]+>')\n","def preprocess_text(sen):\n","    # Removing html tags\n","    sentence = TAG_RE.sub('', sen)\n","    # Remove punctuations and numbers\n","    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n","    # Single character removal\n","    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n","    # Removing multiple spaces\n","    sentence = re.sub(r'\\s+', ' ', sentence)\n","\n","    return sentence\n","\n","def readIMDB(path, seg):\n","    classes = ['pos', 'neg']\n","    data = []\n","    for label in classes:\n","        files = os.listdir(\"drive/MyDrive/二技資管一甲/下學期/news_dataset/\")\n","        for file in files:\n","            with open('aclImdb_v1.tar', 'r', encoding='utf8') as rf:\n","                review = rf.read().replace('\\n', '')\n","                if label == 'pos':\n","                    data.append([preprocess_text(review), 1])\n","                elif label == 'neg':\n","                    data.append([preprocess_text(review), 0])\n","    return data\n","\n","def tokenizer(text):\n","    return [tok.lower() for tok in text.split(' ')] #簡單使用空格來斷詞\n","\n","train_data = readIMDB('aclImdb_v1.tar','train')\n","test_data = readIMDB('aclImdb_v1.tar', 'test')\n","\n","train_tokenized = []\n","test_tokenized = []\n","for review, score in train_data:\n","    train_tokenized.append(tokenizer(review))\n","for review, score in test_data:\n","    test_tokenized.append(tokenizer(review))\n","\n","vocab = set(chain(*train_tokenized)) #把tokenized 所有字給串起來\n","vocab_size = len(vocab)"],"execution_count":12,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d01956fe4b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#簡單使用空格來斷詞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadIMDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb_v1.tar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadIMDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb_v1.tar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-d01956fe4b07>\u001b[0m in \u001b[0;36mreadIMDB\u001b[0;34m(path, seg)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/二技資管一甲/下學期/news_dataset/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb_v1.tar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aclImdb_v1.tar'"]}]},{"cell_type":"code","metadata":{"id":"USYBqVmukhjq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPgJjaWmkmmM"},"source":["### let us see some preprocess result ###\n","text = \"an apple a day, keeps the doctor away\"\n","token = tokenizer(text)\n","print('text: '+text)\n","print('token: '+ str(token))\n","print('encode to index: '+str(encode_samples([token])))\n","text: an apple a day, keeps the doctor away\n","token: ['an', 'apple', 'a', 'day,', 'keeps', 'the', 'doctor', 'away']\n","encode to index: [[23314, 28267, 26943, 0, 49709, 27809, 29103, 70204]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6b94YQUekms1"},"source":["### load word2vec model ###\n","#pre-train model download from: https://github.com/stanfordnlp/GloVe\n","#preprocess:https://stackoverflow.com/questions/51323344/cant-load-glove-6b-300d-txt\n","wvmodel = gensim.models.KeyedVectors.load_word2vec_format('glove.6B.100d.w2vformat.txt',binary=False, encoding='utf-8')\n","\n","## map golve pretrain weight to pytorch embedding pretrain weight\n","embed_size = 100\n","weight = torch.zeros(vocab_size+1, embed_size) #given 0 if the word is not in glove\n","for i in range(len(wvmodel.index2word)):\n","    try:\n","        index = word_to_idx[wvmodel.index2word[i]] #transfer to our word2ind\n","    except:\n","        continue\n","    weight[index, :] = torch.from_numpy(wvmodel.get_vector(wvmodel.index2word[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_hX5Bjykrpj"},"source":["### build model ###\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n","                 bidirectional, weight, labels, **kwargs):\n","        super(RNN, self).__init__(**kwargs)\n","        self.num_hiddens = num_hiddens\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        self.embedding = nn.Embedding.from_pretrained(weight)\n","        self.embedding.weight.requires_grad = False\n","        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n","                               num_layers=num_layers, bidirectional=self.bidirectional,\n","                               dropout=0.3)\n","\n","        if self.bidirectional:\n","            self.linear1 = nn.Linear(num_hiddens * 4, labels)\n","        else:\n","            self.linear1 = nn.Linear(num_hiddens * 2, labels)\n","\n","\n","\n","    def forward(self, inputs):\n","        embeddings = self.embedding(inputs)\n","        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n","        encoding = torch.cat([states[0], states[-1]], dim=1) #if it's bidirectional, choose first and last output\n","        outputs = self.linear1(encoding)\n","\n","        return outputs\n","\n","num_epochs = 10\n","num_hiddens = 100\n","num_layers = 2\n","bidirectional = True\n","labels = 2\n","device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","net = RNN(vocab_size=(vocab_size+1), embed_size=embed_size,\n","                   num_hiddens=num_hiddens, num_layers=num_layers,\n","                   bidirectional=bidirectional, weight=weight,\n","                   labels=labels)\n","\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MV6ErBA-ks5k"},"source":["net.to(device)\n","loss_function = nn.CrossEntropyLoss() # ~ nn.LogSoftmax()+nn.NLLLoss()\n","optimizer = optim.Adam(net.parameters())\n","\n","def train(net,num_epochs,loss_function,optimizer,train_iter,val_iter):\n","    for epoch in range(num_epochs):\n","        start = time.time()\n","        train_loss, val_losses = 0, 0\n","        train_acc, val_acc = 0, 0\n","        n, m = 0, 0\n","        net.train()\n","        for feature, label in train_iter:\n","            n += 1\n","            optimizer.zero_grad()\n","            feature = Variable(feature.to(device))\n","            label = Variable(label.to(device))\n","\n","            score = net(feature)\n","            loss = loss_function(score, label)\n","            loss.backward()\n","            optimizer.step()\n","            train_acc += accuracy_score(torch.argmax(score.cpu().data,dim=1), label.cpu())\n","            train_loss += loss\n","\n","        with torch.no_grad():\n","            net.eval()\n","            for val_feature, val_label in val_iter:\n","                m += 1\n","                val_feature = val_feature.to(device)\n","                val_label = val_label.to(device)\n","                val_score = net(val_feature)\n","                val_loss = loss_function(val_score, val_label)\n","                val_acc += accuracy_score(torch.argmax(val_score.cpu().data,dim=1), val_label.cpu())\n","                val_losses += val_loss\n","\n","        runtime = time.time() - start\n","        print('epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f' %\n","              (epoch, train_loss.data/n, train_acc/n, val_losses.data/m, val_acc/m, runtime))\n","\n","    #save final model\n","    state = {\n","            'epoch': epoch,\n","            'state_dict': net.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","            }\n","    torch.save(state, os.path.join(model_save_path,'last_model.pt'))\n","\n","\n","def predict(net,test_iter):\n","    #state = torch.load(os.path.join(cwd,'checkpoint','epoch10_maxlen300_embed200.pt'),map_location=torch.device('cpu'))\n","    #net.load_state_dict(state['state_dict'])\n","    pred_list = []\n","    true_list = []\n","    softmax = nn.Softmax(dim=1)\n","    with torch.no_grad():\n","        net.eval()\n","        for batch,label in test_iter:\n","            output = net(batch.to(device))\n","            pred_list.extend(torch.argmax(softmax(output),dim=1).cpu().numpy())\n","            true_list.extend(label.cpu().numpy())\n","\n","    acc = accuracy_score(pred_list, true_list)\n","    print('test acc: %f'%acc)\n","\n","    return acc,pred_list,true_list\n","\n","\n","print('start to train...')\n","train(net,num_epochs,loss_function,optimizer,train_iter,val_iter)\n","\n","print('start to predict test set...')\n","acc,pred_list,true_list = predict(net,test_iter)\n","\n","print('Done')"],"execution_count":null,"outputs":[]}]}