{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 674,
     "status": "ok",
     "timestamp": 1621324706900,
     "user": {
      "displayName": "rex tsou",
      "photoUrl": "",
      "userId": "15448100786581130255"
     },
     "user_tz": -480
    },
    "id": "_1WXviv243L2",
    "outputId": "ebbab4bd-8c83-453a-e857-1231bac80055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#參考網站\n",
    "#https://www.kaggle.com/khotijahs1/nlp-financial-news-sentiment-analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_0UWJlL5zF9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/sentiment-analysis-for-financial-news/all-data.csv',delimiter=',',encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqNqFossQJEK"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yafqUp-QKTh"
   },
   "outputs": [],
   "source": [
    "df.index = range(4845)\n",
    "df['Message'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd_SsDW6QLtP"
   },
   "outputs": [],
   "source": [
    "cnt_pro = df['sentiment'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('sentiment', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcpzTmIdQM4g"
   },
   "outputs": [],
   "source": [
    "#Convert sting to numeric\n",
    "sentiment  = {'positive': 0,'neutral': 1,'negative':2} \n",
    "\n",
    "df.sentiment = [sentiment[item] for item in df.sentiment] \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhqXQG_IQOBl"
   },
   "outputs": [],
   "source": [
    "def print_message(index):\n",
    "    example = df[df.index == index][['Message', 'sentiment']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Message:', example[1])\n",
    "print_message(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf5xxzmjQQGi"
   },
   "outputs": [],
   "source": [
    "print_message(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTAB9jHmQRT1"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['Message'] = df['Message'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIhxatnoQSiF"
   },
   "outputs": [],
   "source": [
    "df['Message'] = df['Message'].apply(cleanText)\n",
    "train, test = train_test_split(df, test_size = 0.000001 , random_state = 42)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            #if len(word) < 0:\n",
    "            if len(word) <= 0:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words = tokenize_text(r['Message']), tags = [r.sentiment]), axis = 1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words = tokenize_text(r['Message']), tags = [r.sentiment]), axis = 1)\n",
    "\n",
    "#最大使用數量\n",
    "max_fatures = 500000\n",
    "\n",
    "#最大使用順序\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "#tokenizer語法參考\n",
    "#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer = Tokenizer(num_words = max_fatures, split = ' ', filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower = True)\n",
    "tokenizer.fit_on_texts(df['Message'].values)\n",
    "X = tokenizer.texts_to_sequences(df['Message'].values)\n",
    "X = pad_sequences(X)\n",
    "print('Found %s unique tokens.' % len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fxaihjk_QUil"
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Message'].values)\n",
    "X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2JG0qxwQV_t"
   },
   "outputs": [],
   "source": [
    "#train_tagged.values[2173]\n",
    "train_tagged.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jzgSjZTQXnd"
   },
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(dm = 1, dm_mean = 1, size = 20, window = 8, min_count = 1, workers = 1, alpha = 0.065, min_alpha = 0.065)\n",
    "d2v_model.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGb57A4vQZuw"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples = len(train_tagged.values), epochs = 1)\n",
    "    d2v_model.alpha -= 0.002\n",
    "    d2v_model.min_alpha = d2v_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62ARHjj0QbGm"
   },
   "outputs": [],
   "source": [
    "print(d2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAU5g1dFQdU1"
   },
   "outputs": [],
   "source": [
    "len(d2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhjCREfIQeh2"
   },
   "outputs": [],
   "source": [
    "# save the vectors in a new matrix\n",
    "embedding_matrix = np.zeros((len(d2v_model.wv.vocab) + 1, 20))\n",
    "\n",
    "for i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n",
    "    while i in vec <= 1000:\n",
    "    #print(i)\n",
    "    #print(model.docvecs)\n",
    "          embedding_matrix[i] = vec\n",
    "    #print(vec)\n",
    "    #print(vec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv0eqDGMQgKF"
   },
   "outputs": [],
   "source": [
    "d2v_model.wv.most_similar(positive=['profit'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ81i1lmQhat"
   },
   "outputs": [],
   "source": [
    "d2v_model.wv.most_similar(positive=['investment'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B9KAQDoQiqd"
   },
   "outputs": [],
   "source": [
    "d2v_model.wv.most_similar(positive=['broke'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLIDEOY3Qkc9"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in d2v_model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=250, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjA37i1vQn8N"
   },
   "outputs": [],
   "source": [
    "tsne_plot(d2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McJgLyOZQqvz"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "\n",
    "#模型起始層\n",
    "model = Sequential()\n",
    "\n",
    "#模型加入詞向量層\n",
    "model.add(Embedding(len(d2v_model.wv.vocab) + 1, 20, input_length = X.shape[1], weights = [embedding_matrix], trainable = True))\n",
    "\n",
    "#分割輸入資料\n",
    "def split_input(sequence):\n",
    "     return sequence[:-1], tf.reshape(sequence[1:], (-1, 1))\n",
    "model.add(LSTM(50, return_sequences = False))\n",
    "model.add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "#模型輸出\n",
    "model.summary()\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDq1YDMjQsMV"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuxOXPqNQsi9"
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4ilpBSwQt3f"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "history = model.fit(X_train, Y_train, epochs = 50, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgCQavV6QvP1"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc = 'upper left')\n",
    "plt.show()\n",
    "plt.savefig('model_accuracy.png')\n",
    "\n",
    "#模型Loss數值總覽\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "#plt語法參考\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc = 'upper left')\n",
    "plt.show()\n",
    "plt.savefig('model_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56bI-8XFQxMO"
   },
   "outputs": [],
   "source": [
    "#評估模型\n",
    "_, train_acc = model.evaluate(X_train, Y_train, verbose = 2)\n",
    "_, test_acc = model.evaluate(X_test, Y_test, verbose = 2)\n",
    "print('Train: %.3f, Test: %.4f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjAcVXkwQym2"
   },
   "outputs": [],
   "source": [
    "#預測測試結果\n",
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "print(yhat_probs)\n",
    "\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "print(yhat_classes)\n",
    "\n",
    "#降維至一維語法參考\n",
    "#yhat_probs = yhat_probs[:, 0]\n",
    "#yhat_classes = yhat_classes[:, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_b53PRYQ1uN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rounded_labels=np.argmax(Y_test, axis=1)\n",
    "rounded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pak-yWFDQ29N"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(rounded_labels, yhat_classes)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HgnonfaQ4EL"
   },
   "outputs": [],
   "source": [
    "#混淆矩陣\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "lstm_val = confusion_matrix(rounded_labels, yhat_classes)\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(lstm_val, annot = True, linewidth = 0.7, linecolor = 'cyan', fmt = 'g', ax = ax, cmap = \"BuPu\")\n",
    "plt.title('LSTM Classification Confusion Matrix')\n",
    "plt.xlabel('Y predict')\n",
    "plt.ylabel('Y test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHJRtW1qQ6W-"
   },
   "outputs": [],
   "source": [
    "validation_size = 610\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPXNI7SlQ7xv"
   },
   "outputs": [],
   "source": [
    "model.save('Mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3Hv-Df8Q8MO"
   },
   "outputs": [],
   "source": [
    "message = ['Congratulations! you have won a $1,000 Walmart gift card']\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['0','1','2']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MjVWNkEQ-Ac"
   },
   "outputs": [],
   "source": [
    "message = ['such massive asteroid hit will certainly create new business opportunities']\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['0','1','2']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xt5A2zYXQ_dd"
   },
   "outputs": [],
   "source": [
    "message = ['so does anyone else not open Snapchat anymore ? or is it just me ... ugh this so sad.']\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['0','1','2']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wEqYDiORBL-"
   },
   "outputs": [],
   "source": [
    "message = ['@elonmusk had a terrible experience with a very pushy sales guy from tesla Stanford shop while shopping for model x']\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['0','1','2']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDNxNwEnRCaX"
   },
   "outputs": [],
   "source": [
    "message = ['The local electronics industry is expected to remain stable amid layoff concerns surrounding Japanese electronics giants operating in the country, an official says.']\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['0','1','2']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppSFLSeMRDuV"
   },
   "outputs": [],
   "source": [
    "message = ['The local electronics industry is amid layoff concerns and last year has laid off tens of employees']\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "padded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['0','1','2']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6GNmogJTknkWMCjG1E39d",
   "collapsed_sections": [],
   "name": "finance_senti_anal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
