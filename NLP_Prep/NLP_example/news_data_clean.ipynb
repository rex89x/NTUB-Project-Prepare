{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import re, string, unicodedata\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, \n",
    "accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('', lines = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['category'].nunique())\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['year'].unique())\n",
    "print(df['month'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_merge(x):\n",
    "    if x == 'THE WORLDPOST':\n",
    "        return 'WORLDPOST'\n",
    "    elif x == 'TASTE':\n",
    "        return 'FOOD & DRINK'\n",
    "    elif x == 'STYLE':\n",
    "        return 'STYLE & BEAUTY'\n",
    "    elif x == 'PARENTING':\n",
    "        return 'PARENTS'\n",
    "    elif x == 'COLLEGE':\n",
    "        return 'EDUCATION'\n",
    "    elif x == 'ARTS' or x == 'CULTURE & ARTS':\n",
    "        return 'ARTS & CULTURE'\n",
    "    \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df['category'] = df['category'].apply(category_merge)\n",
    "le = LabelEncoder()\n",
    "data_labels = le.fit_transform(df['category'])\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['authors'].nunique())\n",
    "df['authors'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['authors'] = df['authors'].apply(lambda x: x.split(',')[0])\n",
    "df['authors'] = df['authors'].str.replace(' ', '', regex=False)\n",
    "df['authors'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('authors').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['year'].value_counts().index\n",
    "values = df['year'].value_counts().values\n",
    "\n",
    "colors = df['year']\n",
    "\n",
    "figure = go.Figure(data = [go.Pie(labels = labels, values = values,\n",
    "                              textinfo = \"label+percent\",\n",
    "                              marker = dict(colors = colors))])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['month'].value_counts().index\n",
    "values = df['month'].value_counts().values\n",
    "\n",
    "colors = df['month']\n",
    "\n",
    "figure = go.Figure(data = [go.Pie(labels = labels, values = values,\n",
    "                               textinfo = \"label+percent\",\n",
    "                               marker = dict(colors = colors))])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['category'].value_counts().index\n",
    "values = df['category'].value_counts().values\n",
    "\n",
    "colors = df['category']\n",
    "\n",
    "figure = go.Figure(data = [go.Pie(labels = labels, values = values, \n",
    "                               textinfo = \"label+percent\",\n",
    "                               marker = dict(colors = colors), \n",
    "                               pull=[0, 0, 0.2, 0])])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sizes = df['category'].value_counts().values\n",
    "labels = df['category'].value_counts().index\n",
    "plt.pie(sizes, labels=labels, autopct='%.1f%%',\n",
    "        shadow=True, pctdistance=0.85, labeldistance=1.05, startangle=20, \n",
    "        explode = [0 if i > 0 else 0.2 for i in range(len(sizes))])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=df['category'].value_counts()[:5].index, \n",
    "            x=df['category'].value_counts()[:5].values, orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = data_labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義本文清理\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將short_description本文清理\n",
    "df['short_description'] = df['short_description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(\n",
    "                       width=1600,\n",
    "                       height=800,\n",
    "                       #colormap='PuRd', \n",
    "                       margin=0,\n",
    "                       max_words=500,\n",
    "                       max_font_size=150, min_font_size=30,\n",
    "                       background_color=\"white\").generate(\" \".join(df['short_description']))\n",
    "\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.imshow(word_cloud, interpolation=\"gaussian\")\n",
    "plt.title('WordCloud of short_description', fontsize = 40)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "text = \"I love you, don't you\"\n",
    "\n",
    "tokenizer1 = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer2 = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer3 = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "print(\"Example Text: \", text)\n",
    "print(\"Tokenization by whitespace: \", tokenizer1.tokenize(text))\n",
    "print(\"Tokenization by words using Treebank Word Tokenizer: \", tokenizer2.tokenize(text))\n",
    "print(\"Tokenization by punctuation: \", tokenizer3.tokenize(text))\n",
    "print(\"Tokenization by regular expression: \", tokenizer4.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df['short_description'] = df['short_description'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('Tokenized string:')\n",
    "df['short_description'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [word for word in text if word not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['short_description'] = df['short_description'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How is the Josh\"\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "print(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "print(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    \n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "df['short_description'] = df['short_description'].apply(lambda x : combine_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "   \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    remove_stopwords = [word for word in tokenized_text if word not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(remove_stopwords)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(df['short_description'])\n",
    "\n",
    "print(train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tfidf = tfidf.fit_transform(df['short_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"MultinimialNB\": MultinomialNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df['target'].values.reshape(-1,1))\n",
    "scaler.transform(df['target'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "df['text'] = df.headline + \" \" + df.short_description\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "X = tokenizer.texts_to_sequences(df.text)\n",
    "df['words'] = X\n",
    "\n",
    "\n",
    "df['word_length'] = df.words.apply(lambda i: len(i))\n",
    "df = df[df.word_length >= 5]\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
