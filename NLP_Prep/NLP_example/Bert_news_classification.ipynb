{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 23\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('', lines = True)\n",
    "data.head()\n",
    "\n",
    "data = data.sample(n = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame({\n",
    "    'text':data.headline + data.short_description,\n",
    "    'label':data.category\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "text.label = encoder.fit_transform(text.label)\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(text, test_size=0.1, random_state=23)\n",
    "train, val = train_test_split(train, test_size=0.3,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = 'So give me reason to prove me wrong to wash this memory clean. Let the flood the cross the distance in your eyes.'\n",
    "\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "      sample_txt,\n",
    "      max_length=32,\n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenisation(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, data, targets, tokenizer, max_len):\n",
    "            self.data = data\n",
    "            self.targets = targets\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "            self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "            data = str(self.data[item])\n",
    "            data = \" \".join(data.split())\n",
    "            target = self.targets\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "              data,\n",
    "              add_special_tokens=True,\n",
    "              max_length=self.max_len,\n",
    "              return_token_type_ids=False,\n",
    "              pad_to_max_length=True,\n",
    "              return_attention_mask=True,\n",
    "              return_tensors='pt',\n",
    "            )\n",
    "\n",
    "            ids = encoding['input_ids']\n",
    "            masks = encoding['attention_mask']\n",
    "            token_type_ids = encoding['input_ids']\n",
    "\n",
    "            true_seq_length = len(encoding['input_ids'][0])\n",
    "            pad_size = self.max_len - true_seq_length\n",
    "            pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "            ids = torch.cat((encoding['input_ids'][0], pad_ids))\n",
    "        \n",
    "        return {\n",
    "              'text': data,\n",
    "              'input_ids': ids.flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten(),\n",
    "              'targets': torch.tensor(target[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoader(df, tokenizer, max_len, batch_size):\n",
    "        ds = Tokenisation(\n",
    "        data=df['text'].to_numpy(),\n",
    "        targets=df['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "        )\n",
    "        return DataLoader(ds,batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "train_data_loader = dataLoader(train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = dataLoader(val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = dataLoader(test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "            super(TextClassifier, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "            self.drop = nn.Dropout(p=0.3)\n",
    "            self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "            \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "            _, pooled_output = self.bert(\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask\n",
    "            )\n",
    "            output = self.drop(pooled_output)\n",
    "            return self.out(output)\n",
    "    \n",
    "    def unfreeze(self,start_layer,end_layer):\n",
    "            def children(m):\n",
    "                return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "            def set_trainable_attr(m, b):\n",
    "                m.trainable = b\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = b\n",
    "            def apply_leaf(m, f):\n",
    "                c = children(m)\n",
    "                if isinstance(m, nn.Module):\n",
    "                    f(m)\n",
    "                if len(c) > 0:\n",
    "                    for l in c:\n",
    "                        apply_leaf(l, f)\n",
    "            def set_trainable(l, b):\n",
    "                apply_leaf(l, lambda m: set_trainable_attr(m, b))\n",
    "\n",
    "            set_trainable(self.bert, False)\n",
    "            for i in range(start_layer, end_layer+1):\n",
    "                set_trainable(self.bert.encoder.layer[i], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(len(text.label.unique()))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "MAX_LENGTH = 128\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "LearningRate = 5e-5\n",
    "\n",
    "BETAS = (0.9, 0.999)\n",
    "BERT_WEIGHT_DECAY = 0.01\n",
    "EPS = 1e-8\n",
    "\n",
    "bert_identifiers = ['embedding', 'encoder', 'pooler']\n",
    "no_weight_decay_identifiers = ['bias', 'LayerNorm.weight']\n",
    "grouped_model_parameters = [\n",
    "        {'params': [param for name, param in model.named_parameters()\n",
    "                    if any(identifier in name for identifier in bert_identifiers) and\n",
    "                    not any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
    "        'lr': LearningRate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': BERT_WEIGHT_DECAY,\n",
    "        'eps': EPS},\n",
    "        {'params': [param for name, param in model.named_parameters()\n",
    "                    if any(identifier in name for identifier in bert_identifiers) and\n",
    "                    any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
    "        'lr': LearningRate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': 0.0,\n",
    "        'eps': EPS},\n",
    "        {'params': [param for name, param in model.named_parameters()\n",
    "                    if not any(identifier in name for identifier in bert_identifiers)],\n",
    "        'lr': LearningRate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': 0.0,\n",
    "        'eps': EPS}\n",
    "]\n",
    "\n",
    "optimizers = AdamW(grouped_model_parameters)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    \n",
    "    \n",
    "        model = model.train()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "        for d in tqdm(data_loader):\n",
    "                input_ids = d[\"input_ids\"].to(device)\n",
    "                attention_mask = d[\"attention_mask\"].to(device)\n",
    "                targets = d[\"targets\"].to(device)\n",
    "                outputs = model(\n",
    "                  input_ids=input_ids,\n",
    "                  attention_mask=attention_mask\n",
    "                )\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                correct_predictions += torch.sum(preds == targets)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    \n",
    "        model = model.eval()\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "                for d in tqdm(data_loader):\n",
    "                        input_ids = d[\"input_ids\"].to(device)\n",
    "                        attention_mask = d[\"attention_mask\"].to(device)\n",
    "                        targets = d[\"targets\"].to(device)\n",
    "\n",
    "                        outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                        )\n",
    "\n",
    "                        _, preds = torch.max(outputs, dim=1)\n",
    "                        loss = loss_fn(outputs, targets)\n",
    "                        correct_predictions += torch.sum(preds == targets)\n",
    "                        losses.append(loss.item())\n",
    "\n",
    "        return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "        print('-' * 10)\n",
    "        train_acc, train_loss = train_epoch(\n",
    "                                        model,\n",
    "                                        train_data_loader,\n",
    "                                        loss_fn,\n",
    "                                        optimizer,\n",
    "                                        device,\n",
    "                                        scheduler,\n",
    "                                        len(train)\n",
    "        )\n",
    "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "        val_acc, val_loss = eval_model(\n",
    "                                        model,\n",
    "                                        val_data_loader,\n",
    "                                        loss_fn,\n",
    "                                        device,\n",
    "                                        len(val)\n",
    "        )\n",
    "        \n",
    "        print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "        print()\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "                torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "                best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
